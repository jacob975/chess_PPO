{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import sac\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, rank, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the inital seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"BipedalWalker-v3\"\n",
    "num_cpu = 10  # Number of processes to use\n",
    "env = make_vec_env(env_id, n_envs=num_cpu, seed=0, vec_env_cls=DummyVecEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Create a SAC agent\n",
    "agent = sac.SAC('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.1     |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 719      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 1550     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.39    |\n",
      "|    critic_loss     | 121      |\n",
      "|    ent_coef        | 0.958    |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 144      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 292      |\n",
      "|    ep_rew_mean     | -104     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 639      |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.4    |\n",
      "|    critic_loss     | 0.658    |\n",
      "|    ent_coef        | 0.623    |\n",
      "|    ent_coef_loss   | -3.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 582      |\n",
      "|    ep_rew_mean     | -100     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 640      |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 20120    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17      |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    ent_coef        | 0.551    |\n",
      "|    ent_coef_loss   | -3.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 596      |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 36120    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.4    |\n",
      "|    critic_loss     | 0.567    |\n",
      "|    ent_coef        | 0.341    |\n",
      "|    ent_coef_loss   | -6.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 540      |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 50420    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.5    |\n",
      "|    critic_loss     | 0.478    |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -9.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5031     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | -95.8    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 514      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.6    |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -12      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6389     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | -95.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 510      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 66580    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.8    |\n",
      "|    critic_loss     | 0.272    |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -12.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6647     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | -96.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 523      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.2    |\n",
      "|    critic_loss     | 0.356    |\n",
      "|    ent_coef        | 0.0929   |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7989     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 926      |\n",
      "|    ep_rew_mean     | -96.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 535      |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 87280    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.8    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0751   |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8717     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 993      |\n",
      "|    ep_rew_mean     | -95.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 558      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 103280   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.1    |\n",
      "|    critic_loss     | 0.45     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10317    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+03 |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 563      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 115110   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.3    |\n",
      "|    critic_loss     | 0.13     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -19.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.08e+03 |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 566      |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 117440   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.1    |\n",
      "|    critic_loss     | 0.209    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -19.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11733    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 993      |\n",
      "|    ep_rew_mean     | -96.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 571      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 120730   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.6    |\n",
      "|    critic_loss     | 0.851    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12062    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | -97      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 586      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 133520   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.2    |\n",
      "|    critic_loss     | 0.29     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -17.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13341    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | -96.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 590      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 137200   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.2    |\n",
      "|    critic_loss     | 0.773    |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | -15.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13709    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train on the environment save the best model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m      3\u001b[0m     total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1000000\u001b[39;49m, \n\u001b[0;32m      4\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[0;32m      5\u001b[0m     tb_log_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSAC_BipedalWalker-v3_100k\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:299\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    290\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[0;32m    291\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    303\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    304\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    305\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    306\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:353\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    352\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 353\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    355\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    357\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:207\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    203\u001b[0m actor_losses, critic_losses \u001b[39m=\u001b[39m [], []\n\u001b[0;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m gradient_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gradient_steps):\n\u001b[0;32m    206\u001b[0m     \u001b[39m# Sample replay buffer\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     replay_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer\u001b[39m.\u001b[39;49msample(batch_size, env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vec_normalize_env)\n\u001b[0;32m    209\u001b[0m     \u001b[39m# We need to sample because `log_std` may have changed between two gradient steps\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sde:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:286\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39mSample elements from the replay buffer.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mCustom sampling when using memory efficient variant,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_memory_usage:\n\u001b[1;32m--> 286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msample(batch_size\u001b[39m=\u001b[39;49mbatch_size, env\u001b[39m=\u001b[39;49menv)\n\u001b[0;32m    287\u001b[0m \u001b[39m# Do not sample the element with index `self.pos` as the transitions is invalid\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# (we use only one array to store `obs` and `next_obs`)\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:111\u001b[0m, in \u001b[0;36mBaseBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    109\u001b[0m upper_bound \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos\n\u001b[0;32m    110\u001b[0m batch_inds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, upper_bound, size\u001b[39m=\u001b[39mbatch_size)\n\u001b[1;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_samples(batch_inds, env\u001b[39m=\u001b[39;49menv)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:297\u001b[0m, in \u001b[0;36mReplayBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_samples\u001b[39m(\u001b[39mself\u001b[39m, batch_inds: np\u001b[39m.\u001b[39mndarray, env: Optional[VecNormalize] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ReplayBufferSamples:\n\u001b[0;32m    296\u001b[0m     \u001b[39m# Sample randomly the env idx\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     env_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_envs, size\u001b[39m=\u001b[39;49m(\u001b[39mlen\u001b[39;49m(batch_inds),))\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_memory_usage:\n\u001b[0;32m    300\u001b[0m         next_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_obs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[(batch_inds \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size, env_indices, :], env)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train on the environment save the best model\n",
    "agent.learn(\n",
    "    total_timesteps=1000000, \n",
    "    log_interval=10, \n",
    "    tb_log_name=\"SAC_BipedalWalker-v3_100k\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "agent.save(\"SAC_BipedalWalker-v3_100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "agent = sac.SAC.load(\"SAC_BipedalWalker-v3_100k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\gym_chess\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] 執行緒模式設定後就不能再變更它。\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent with animation\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action, _states = agent.predict(obs, deterministic=False)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "\n",
    "input()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
